{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding an Autokey Cipher with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A project by Sam Greydanus. November 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataloader import Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'max_key_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ee42148fe38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkey_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sam/code_ideas/crypto-nn/autokey-nn/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alphabet, tsteps, key_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphabet\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_key_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_key_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'max_key_len' is not defined"
     ]
    }
   ],
   "source": [
    "A = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "tsteps = 7\n",
    "max_key_len = 5\n",
    "\n",
    "dataloader = Dataloader(A, tsteps=tsteps, max_key_len=max_key_len)\n",
    "dataloader.next_batch(10, info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for generating Autokey training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Autokey():\n",
    "    def __init__(self, alphabet, tsteps, key_len):\n",
    "        self.A = alphabet\n",
    "        self.tsteps = tsteps\n",
    "        self.key_len = key_len\n",
    "    \n",
    "    def encode(self, key, plain):\n",
    "        cipher = '' ; key = copy.deepcopy(key)\n",
    "        for i in range(len(plain)):\n",
    "            plain_ix = self.A.find(plain[i])\n",
    "            shift = self.A.find(key[i])\n",
    "            assert plain_ix >=0, 'plaintext characters must exist in alphabet'\n",
    "            assert shift >=0, 'key characters must exist in alphabet'\n",
    "            cipher_ix = (plain_ix + shift) % len(self.A)\n",
    "            cipher += self.A[cipher_ix]\n",
    "            key += self.A[plain_ix]\n",
    "        return cipher\n",
    "            \n",
    "    def decode(self, key, cipher):\n",
    "        plain = '' ; key = copy.deepcopy(key)\n",
    "        for i in range(len(cipher)):\n",
    "            cipher_ix = self.A.find(cipher[i])\n",
    "            shift = self.A.find(key[i])\n",
    "            assert cipher_ix >=0, 'ciphertext characters must exist in alphabet'\n",
    "            assert shift >=0, 'key characters must exist in alphabet'\n",
    "            plain_ix = (cipher_ix - shift) % len(self.A)\n",
    "            plain += self.A[plain_ix]\n",
    "            key += self.A[plain_ix]\n",
    "        return plain\n",
    "    \n",
    "    def rands(self, size):\n",
    "        ix = np.random.randint(len(self.A),size=size)\n",
    "        return ''.join([self.A[i] for i in ix])\n",
    "    \n",
    "    def one_hot(self, s):\n",
    "        ix = [self.A.find(l) for l in s]\n",
    "        z = np.zeros((len(s),len(self.A)))\n",
    "        z[range(len(s)),ix] = 1\n",
    "        return z\n",
    "    \n",
    "    def next_batch(self, batch_size, verbose=False):\n",
    "        batch_X = [] ; batch_y = [] ; batch_Xs = [] ; batch_ks = [] ; batch_ys = []\n",
    "        for _ in range(batch_size):\n",
    "            ys = self.rands(self.tsteps)\n",
    "            ks = self.rands(self.key_len)\n",
    "            Xs = self.encode(ks, ys)\n",
    "            if verbose: print Xs, ks, ys\n",
    "            X = self.one_hot(ks + Xs)\n",
    "            y = self.one_hot(ks + ys)\n",
    "            batch_X.append(X)\n",
    "            batch_y.append(y)\n",
    "            batch_Xs.append(Xs) ; batch_ks.append(ks) ; batch_ys.append(ys)\n",
    "        \n",
    "        if not verbose:\n",
    "            return (batch_X,batch_y)\n",
    "        else:\n",
    "            return (batch_X, batch_y, batch_Xs, batch_ks, batch_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful modular implementation of a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SequentialModel():\n",
    "    def __init__(self, batch_size, tsteps, xlen, ylen, lr):\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.batch_size = batch_size\n",
    "        self.tsteps = tsteps\n",
    "        self.xlen = xlen\n",
    "        self.ylen = ylen\n",
    "        self.lr = lr\n",
    "        self.x = x = tf.placeholder(tf.float32, shape=[None, None, xlen], name=\"x\")\n",
    "        self.y = y = tf.placeholder(tf.float32, shape=[None, None, ylen], name=\"y\")\n",
    "\n",
    "        self.params = params = {}\n",
    "        print 'hidden layer dim: ', xlen\n",
    "        self.fc1_size = fc1_size = xlen\n",
    "        self.rnn_size = rnn_size = 200\n",
    "        with tf.variable_scope('model',reuse=False):\n",
    "            xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(ylen), dtype=tf.float32)\n",
    "            params['W1'] = tf.get_variable(\"W1\", [xlen, fc1_size], initializer=xavier_l1)\n",
    "\n",
    "            rnn_init = tf.truncated_normal_initializer(stddev=0.075, dtype=tf.float32)\n",
    "            params['rnn'] = tf.nn.rnn_cell.LSTMCell(rnn_size, state_is_tuple=True, initializer=rnn_init)\n",
    "\n",
    "            params['istate_batch'] = params['rnn'].zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "            params['istate'] = params['rnn'].zero_state(batch_size=1, dtype=tf.float32)\n",
    "\n",
    "            xavier_l3 = tf.truncated_normal_initializer(stddev=1./np.sqrt(rnn_size), dtype=tf.float32)\n",
    "            params['W3'] = tf.get_variable(\"W3\", [rnn_size, ylen], initializer=xavier_l3)\n",
    "            \n",
    "        y_hat_batch, params['fstate_batch'] = self.forward(params['istate_batch'], tsteps=tsteps, reuse=False)\n",
    "        self.y_hat, params['fstate'] = self.forward(params['istate'], tsteps=1, reuse=True)\n",
    "        \n",
    "        self.loss = tf.nn.l2_loss(y - y_hat_batch) / batch_size\n",
    "        self.optimizer = tf.train.AdamOptimizer(lr)\n",
    "        self.grads = self.optimizer.compute_gradients(self.loss, var_list=tf.trainable_variables())\n",
    "        self.train_op = self.optimizer.apply_gradients(self.grads)\n",
    "\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        self.c = self.params['istate'].c.eval()\n",
    "        self.h = self.params['istate'].h.eval()\n",
    "            \n",
    "    def forward(self, state, tsteps, reuse=False):\n",
    "        with tf.variable_scope('model', reuse=reuse):\n",
    "            x = tf.reshape(self.x, [-1, self.xlen])\n",
    "            h = tf.matmul(x, self.params['W1'])\n",
    "            h = tf.nn.relu(h) # ReLU nonlinearity\n",
    "\n",
    "            hs = [tf.squeeze(h_, [1]) for h_ in tf.split(1, tsteps, tf.reshape(h, [-1, tsteps, self.fc1_size]))]\n",
    "            rnn_outs, state = tf.nn.seq2seq.rnn_decoder(hs, state, self.params['rnn'], scope='model')\n",
    "            rnn_out = tf.reshape(tf.concat(1, rnn_outs), [-1, self.rnn_size])\n",
    "\n",
    "            logps = tf.matmul(rnn_out, self.params['W3'])\n",
    "            logps = tf.nn.softmax(logps)\n",
    "            p = tf.reshape(logps, [-1, tsteps, self.ylen])\n",
    "        return p, state\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        feed = {self.x: batch[0], self.y: batch[1]}\n",
    "        train_loss, _ = self.sess.run([self.loss, self.train_op], feed_dict=feed)\n",
    "        return train_loss\n",
    "    \n",
    "    def step(self, x):\n",
    "        feed = {self.x: x, self.params['istate'].c: self.c, self.params['istate'].h: self.h}\n",
    "        fetch = [self.y_hat, self.params['fstate'].c, self.params['fstate'].h]\n",
    "        [y_hat, self.c, self.h] = self.sess.run(fetch, feed)\n",
    "        return y_hat\n",
    "    \n",
    "    def generate(self, steps_forward):\n",
    "        prev_x = np.asarray([[[0,1]]]) #np.zeros((1,1,self.xlen))\n",
    "        xs = np.zeros((1,0,self.ylen))\n",
    "\n",
    "        for t in range(steps_forward):\n",
    "            xs = np.concatenate((xs,prev_x), axis=1)\n",
    "            prev_x = self.step(prev_x)\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer dim:  26\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "save_path = 'models/model.ckpt'\n",
    "A = 'abcdefghijklmnopqrstuvwxyz'\n",
    "tsteps = 10\n",
    "key_len = 5\n",
    "autokey = Autokey(A, tsteps=tsteps, key_len=key_len)\n",
    "model = SequentialModel(batch_size=32, tsteps=tsteps+key_len, xlen=len(A), ylen=len(A), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of TensorFlow parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model overview:\n",
      "\tvariable \"model/W1:0\" has 676 parameters\n",
      "\tvariable \"model/W3:0\" has 5200 parameters\n",
      "\tvariable \"model/model/LSTMCell/W_0:0\" has 180800 parameters\n",
      "\tvariable \"model/model/LSTMCell/B:0\" has 800 parameters\n",
      "Total of 187476 parameters\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0 ; print \"Model overview:\"\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print '\\tvariable \"{}\" has {} parameters' \\\n",
    "        .format(variable.name, variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print \"Total of {} parameters\".format(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tstep 30000, loss 1.261393, batch time 0.044088\n",
      "\tstep 30100, loss 1.373854, batch time 0.035164\n",
      "\tstep 30200, loss 1.339942, batch time 0.036970\n",
      "\tstep 30300, loss 1.187450, batch time 0.035823\n",
      "\tstep 30400, loss 1.308978, batch time 0.038308\n",
      "\tstep 30500, loss 1.119762, batch time 0.045648\n",
      "\tstep 30600, loss 1.254778, batch time 0.050540\n",
      "\tstep 30700, loss 1.128731, batch time 0.039066\n",
      "\tstep 30800, loss 1.220700, batch time 0.038307\n",
      "\tstep 30900, loss 1.227064, batch time 0.039420\n",
      "\tstep 31000, loss 1.116932, batch time 0.034871\n",
      "\tstep 31100, loss 1.065990, batch time 0.034955\n",
      "\tstep 31200, loss 1.067873, batch time 0.033892\n",
      "\tstep 31300, loss 0.968747, batch time 0.042924\n",
      "\tstep 31400, loss 0.801423, batch time 0.037802\n",
      "\tstep 31500, loss 1.065521, batch time 0.039608\n",
      "\tstep 31600, loss 0.923276, batch time 0.037412\n",
      "\tstep 31700, loss 0.886054, batch time 0.043036\n",
      "\tstep 31800, loss 1.013661, batch time 0.035664\n",
      "\tstep 31900, loss 0.941201, batch time 0.039171\n",
      "\tstep 32000, loss 1.007557, batch time 0.035507\n",
      "\tstep 32100, loss 0.861117, batch time 0.046550\n",
      "\tstep 32200, loss 0.751682, batch time 0.038227\n",
      "\tstep 32300, loss 0.720104, batch time 0.035453\n",
      "\tstep 32400, loss 0.830814, batch time 0.036470\n",
      "\tstep 32500, loss 0.668823, batch time 0.056851\n",
      "\tstep 32600, loss 0.646005, batch time 0.050808\n",
      "\tstep 32700, loss 0.738920, batch time 0.037443\n",
      "\tstep 32800, loss 0.683676, batch time 0.042477\n",
      "\tstep 32900, loss 0.652699, batch time 0.037416\n",
      "\tstep 33000, loss 0.696283, batch time 0.035076\n",
      "\tstep 33100, loss 0.877998, batch time 0.034999\n",
      "\tstep 33200, loss 0.561641, batch time 0.040441\n",
      "\tstep 33300, loss 0.525270, batch time 0.034393\n",
      "\tstep 33400, loss 0.592153, batch time 0.037303\n",
      "\tstep 33500, loss 0.552083, batch time 0.036809\n",
      "\tstep 33600, loss 0.473374, batch time 0.038336\n",
      "\tstep 33700, loss 0.597712, batch time 0.037460\n",
      "\tstep 33800, loss 0.442194, batch time 0.040875\n",
      "\tstep 33900, loss 0.452147, batch time 0.035971\n",
      "\tstep 34000, loss 0.285565, batch time 0.035559\n",
      "\tstep 34100, loss 0.395117, batch time 0.035059\n",
      "\tstep 34200, loss 0.235503, batch time 0.041621\n",
      "\tstep 34300, loss 0.214385, batch time 0.041189\n",
      "\tstep 34400, loss 0.229615, batch time 0.043568\n",
      "\tstep 34500, loss 0.251712, batch time 0.036135\n",
      "\tstep 34600, loss 0.185488, batch time 0.038005\n",
      "\tstep 34700, loss 0.202404, batch time 0.034695\n",
      "\tstep 34800, loss 0.231681, batch time 0.036723\n",
      "\tstep 34900, loss 0.165877, batch time 0.041616\n",
      "\tstep 35000, loss 0.150973, batch time 0.036799\n",
      "\tstep 35100, loss 0.130619, batch time 0.035951\n",
      "\tstep 35200, loss 0.112777, batch time 0.052632\n",
      "\tstep 35300, loss 0.093493, batch time 0.041440\n",
      "\tstep 35400, loss 0.121190, batch time 0.037067\n",
      "\tstep 35500, loss 0.130769, batch time 0.035529\n",
      "\tstep 35600, loss 0.091647, batch time 0.038091\n",
      "\tstep 35700, loss 0.124923, batch time 0.035403\n",
      "\tstep 35800, loss 0.261434, batch time 0.034193\n",
      "\tstep 35900, loss 0.122706, batch time 0.034401\n",
      "\tstep 36000, loss 0.050391, batch time 0.036429\n",
      "\tstep 36100, loss 0.098334, batch time 0.037828\n",
      "\tstep 36200, loss 0.063696, batch time 0.035385\n",
      "\tstep 36300, loss 0.066629, batch time 0.036812\n",
      "\tstep 36400, loss 0.125230, batch time 0.040608\n",
      "\tstep 36500, loss 0.052746, batch time 0.035954\n",
      "\tstep 36600, loss 0.029794, batch time 0.059291\n",
      "\tstep 36700, loss 0.051132, batch time 0.068490\n",
      "\tstep 36800, loss 0.067172, batch time 0.047636\n",
      "\tstep 36900, loss 0.034155, batch time 0.035125\n",
      "\tstep 37000, loss 0.060958, batch time 0.036482\n",
      "\tstep 37100, loss 0.040871, batch time 0.037900\n",
      "\tstep 37200, loss 0.053490, batch time 0.045916\n",
      "\tstep 37300, loss 0.053277, batch time 0.059382\n",
      "\tstep 37400, loss 0.027302, batch time 0.034506\n",
      "\tstep 37500, loss 0.057811, batch time 0.036028\n",
      "\tstep 37600, loss 0.057801, batch time 0.039790\n",
      "\tstep 37700, loss 0.114404, batch time 0.036344\n",
      "\tstep 37800, loss 0.039248, batch time 0.036268\n",
      "\tstep 37900, loss 0.030286, batch time 0.043417\n",
      "\tstep 38000, loss 0.038271, batch time 0.044495\n",
      "\tstep 38100, loss 0.068949, batch time 0.038544\n",
      "\tstep 38200, loss 0.032912, batch time 0.043310\n",
      "\tstep 38300, loss 0.023254, batch time 0.035634\n",
      "\tstep 38400, loss 0.025825, batch time 0.038251\n",
      "\tstep 38500, loss 0.068949, batch time 0.035140\n",
      "\tstep 38600, loss 0.029889, batch time 0.036811\n",
      "\tstep 38700, loss 0.018712, batch time 0.039324\n",
      "\tstep 38800, loss 0.057980, batch time 0.047288\n",
      "\tstep 38900, loss 0.021470, batch time 0.035625\n",
      "\tstep 39000, loss 0.020226, batch time 0.036466\n",
      "\tstep 39100, loss 0.018490, batch time 0.037619\n",
      "\tstep 39200, loss 0.014743, batch time 0.037285\n",
      "\tstep 39300, loss 0.042171, batch time 0.035822\n",
      "\tstep 39400, loss 0.037646, batch time 0.037154\n",
      "\tstep 39500, loss 0.018596, batch time 0.035373\n",
      "\tstep 39600, loss 0.031267, batch time 0.035710\n",
      "\tstep 39700, loss 0.014755, batch time 0.037436\n",
      "\tstep 39800, loss 0.014410, batch time 0.040079\n",
      "\tstep 39900, loss 0.011313, batch time 0.033943\n",
      "\n",
      "total runtime: 401.921972 seconds\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "for i in range(global_step, 40000):\n",
    "    start = time.time()\n",
    "    \n",
    "    batch = autokey.next_batch(model.batch_size)\n",
    "    train_loss = model.train_step(batch)\n",
    "    \n",
    "    if global_step%100 == 0:\n",
    "        print \"\\tstep {}, loss {:3f}, batch time {:3f}\".format(i, train_loss, time.time()-start)\n",
    "    global_step += 1\n",
    "        \n",
    "print \"\\ntotal runtime: {:3f} seconds\".format(time.time()-begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kuzzufjzle dpevk hfvekyeehu\n",
      "hfvek \n",
      "hfvekyeehu\n",
      "[1 1 1 1 1]\n",
      "100.0 % correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:12: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "batch = autokey.next_batch(1, verbose=True)\n",
    "\n",
    "model.c = model.params['istate'].c.eval()\n",
    "model.h = model.params['istate'].h.eval()\n",
    "\n",
    "cipher = ''\n",
    "for i in range(tsteps):\n",
    "    p = np.tile(batch[0][0][i,:],[1,1,1])\n",
    "    c = model.step(p)\n",
    "#     print y_hat[0,0,:]\n",
    "    ix = np.where(c[0,0,:] == np.amax(c[0,0,:]))\n",
    "    cipher += autokey.A[ix[0]]\n",
    "cipher = cipher[autokey.key_len:]\n",
    "print cipher, '\\n', batch[4][0]\n",
    "t = np.asarray([c==batch[4][0][i] for (i,c) in enumerate(cipher)])\n",
    "print t + 0\n",
    "print \"{} % correct\".format(float(np.sum(t))/np.shape(t)[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bozoszctyfmkhygveaucvvftxk\n",
      "decoded sequence is: 'youknownothinggonfnrw'\n",
      "NOTE: errors appear towards end of sequence where it exceeds # time steps used for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:14: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "plaintext = 'youknownothingjonsnow'\n",
    "key = 'bozos'\n",
    "ciphertext = key + autokey.encode(key, plaintext)\n",
    "print ciphertext\n",
    "\n",
    "model.c = model.params['istate'].c.eval()\n",
    "model.h = model.params['istate'].h.eval()\n",
    "\n",
    "decoded = ''\n",
    "for i in range(len(ciphertext)):\n",
    "    p = np.tile(autokey.one_hot(ciphertext[i]),[1,1,1])\n",
    "    c = model.step(p)\n",
    "    ix = np.where(c[0,0,:] == np.amax(c[0,0,:]))\n",
    "    decoded += autokey.A[ix[0]]\n",
    "decoded = decoded[autokey.key_len:]\n",
    "print \"decoded sequence is: '{}'\".format(decoded)\n",
    "print \"NOTE: errors appear towards end of sequence where it exceeds # time steps used for training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
